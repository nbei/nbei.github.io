<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>STransGAN: An Empirical Study on Transformer in GANs</title>

	<!-- Meta tags for search engines to crawl -->
	<meta name="robots" content="index,follow">
	<meta name="description" content="STransGAN">
	<meta name="keywords" content="stransgan, transformer, gan, swin">

	<!-- Fonts and stuff -->
	<link href=" ./gan-pos-encoding/css" rel="stylesheet" type="text/css">
	<link rel="stylesheet" type="text/css" href="./gan-pos-encoding/project.css" media="screen">
	<link rel="stylesheet" type="text/css" media="screen" href="./gan-pos-encoding/iconize.css">
	<script type="text/javascript" async="" src="./gan-pos-encoding/ga.js.download"></script>
	<script async="" src="./gan-pos-encoding/prettify.js.download"></script>

	<script type="text/javascript">

		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-22940424-1']);
		_gaq.push(['_trackPageview']);

		(function () {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();

	</script>

</head>

<body>
	<div id="content">
		<div id="content-inner">

			<div class="section head">
				<h1>STransGAN: An Empirical Study on Transformer in GANs</h1>

				<div class="authors">
					<a href="https://nbei.github.io/">Rui
						Xu<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://sites.google.com/view/xiangyuxu">Xiangyu
						Xu<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="http://chenkai.site/#home">Kai
						Chen<sup>3,4</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy<sup>2</sup></a>
				</div>

				<div class="affiliations">
					<sup>1</sup> <a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">CUHK - SenseTime Joint Lab, The
						Chinese University of Hong Kong</a> <br>
					<sup>2</sup> <a href="https://www.mmlab-ntu.com/index.html">S-Lab, Nanyang Technological University, Singapore</a> <br>
					<sup>3</sup> <a>SenseTime Research</a>
					<sup>4</sup> <a>Shanghai AI Laboratory</a>
				</div>
				<div class="venue"></div>
			</div>

			<div class="section introduction">
				<h2>Abstract</h2>
				<center><img src="./stransgan/teaser.png" border="0" width="95%"></center>
				<br>
				<p>
					Transformer becomes prevalent in computer vision, especially for high-level vision tasks.
					However, deploying Transformer in the generative adversarial network (GAN) framework is still an open yet challenging problem.
					In this paper, we conduct a comprehensive empirical study to investigate the intrinsic properties of Transformer in GAN for high-fidelity image synthesis.
					Our analysis highlights the importance of feature locality in image generation.
					We first investigate the effective ways to implement local attention.
					We then examine the influence of residual connections in self-attention layers and propose a novel way to reduce their negative impacts on learning discriminators and conditional generators.
					Our study leads to a new design of Transformers in GAN, a convolutional neural network (CNN)-free generator termed as <b>STrans-G</b>, which achieves competitive results in both unconditional and conditional image generations.
					The Transformer-based discriminator, <b>STrans-D</b>, also significantly reduces its gap against the CNN-based discriminators.
				</p>
			</div>

			<div class="section analysis">
				<h2>STrans-G</h2>
				<center><img src="./stransgan/g-arch.png" border="0" width="95%"></center>
				<p>
					We start from a straightforward baseline structure, Trans-G, which is composed of standard vision Transformer blocks, as shown in the figure.
					Samples generated by Trans-G, however, often contain severe artifacts and unrealistic details, leading to poor visual quality.
					Through analyzing the intrinsic behavior of the attention layers, we find the global attention always breaks the locality of image data,
					especially when synthesizing high-resolution features. The finding motivates us to explore the effect of various local attention mechanisms in generating realistic high-resolution images.
					After a careful comparison of different local attention mechanisms, we finally choose Swin architecture as the building block to construct a CNN-free generator, STrans-G.
					The further analysis on the attention distance clearly shows the difference between the global attention and the local attention. 
				</p>
				<center><img src="./stransgan/locality.png" border="0" width="95%"></center>
			</div>

			<div class="section analysis">
				<h2>STrans-D</h2>
				<center><img src="./stransgan/stransd-revise.png" border="0" width="95%"></center>
				<p>
					Transformer employs a residual connection around each sub-layer of self-attention and the pointwise fully connected layer.
					Through a detailed analysis of norm ratios, we find that residual connections tend to dominate the information flow in a Transformer-based discriminator. Consequently, sub-layers that perform self-attention and fully connected operations in the discriminator are inadvertently bypassed, causing inferior quality and slow convergence during training.  
					We address this problem by replacing each residual connection with a skip-projection layer, which better retains the information flow in the residual blocks. 
					
				</p>
			</div>

			<div class="section analysis">
				<h2>AdaNorm-T</h2>
				<center><img src="./stransgan/c-strans-g.png" border="0" width="90%"></center>
				<p>
					We observe that conventional approaches of injecting conditional class information do not work well for Transformer-based conditional GAN.
					The main culprit lies in the large information flow through the residual connections in the Transformer generator.
					If the conditional information is injected within the main branch(Also known as the residual mapping path in ResNet.),
					it will largely be ignored and contribute little to the final outputs.
					We present a viable way of adopting conditional normalization layers in the trunk, which helps retain conditional information throughout the Transformer generator.
				</p>
			</div>

			<div class="section experiments">
				<h2>Results</h2>
				<br>
				<center><img src="./stransgan/res-table.png" border="0" width="95%"></center>
				<p>
					In unconditional generation, STrans-G significantly outperforms all previous methods in CelebA 64<sup>2</sup>.
					It also achieves competitive performance in the high-resolution setting of FFHQ 256<sup>2</sup>.
					For conditional image generation, with the proposed AdaIN-T layer, STrans-G improves the SOTA Inception Score from 10.14 to 11.62 on CIFAR10.
					Besides, for the first time, our study shows the potential of Transformers in the challenging ImageNet dataset.
				</p>

				<center><img src="./stransgan/visual.png" border="0" width="95%"></center>
				<p>
					The visual quality of these generated samples suggests the great potential of using pure Transformer blocks in GANs.
				</p>
			</div>

			<div class="section materials">
				<h2>Materials</h2>
				<center>
					<ul>

						<li class="grid">
							<div class="griditem">
								<a href="" target="_blank" class="imageLink"><img
										src="./stransgan/paper-small.png" border="0" width="50%"></a><br>
								<a href="">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Full Paper
									with
									Appendix</a>
							</div>
						</li>
						&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

						<li class="grid">
							<div class="griditem">
								<a href=""><img src="./gan-pos-encoding/code.png"></a><br>
								<a href="">Codes will come soon.</a>
							</div>
						</li>

					</ul>
				</center>
			</div>

			<br>

			<div class="section citation">
				<h2>Citation</h2>
				<div class="section bibtex">
					<pre>@InProceedings{xu_2021_stransgan,
author = {Xu, Rui and Xu, Xiangyu and Chen, Kai and Zhou, Bolei and Loy, Chen Change},
title = {STransGAN: An Empirical Study on Transformer in GANs},
booktitle = {arxiv},
month = {October},
year = {2021}
}</pre>
				</div>
			</div>

		</div>
		<!--=================Contact==========================-->
		<div class="section contact">
			<h2 id="contact">Contact</h2>
			<p>If you have any question, please contact Rui Xu at <strong>xr018@ie.cuhk.edu.hk</strong>.</p> <br>
		</div>
	</div>

</body>

</html>