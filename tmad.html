<!DOCTYPE html
	PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Positional Encoding as Spatial Inductive Bias in GANs</title>

	<!-- Meta tags for search engines to crawl -->
	<meta name="robots" content="index,follow">
	<meta name="description" content="Positional Encoding in GANs">
	<meta name="keywords" content="positional encoding, gan, spatial inductive bias, stylegan2">

	<!-- Fonts and stuff -->
	<link href=" ./tmad/css" rel="stylesheet" type="text/css">
	<link rel="stylesheet" type="text/css" href="./tmad/project.css" media="screen">
	<link rel="stylesheet" type="text/css" media="screen" href="./tmad/iconize.css">
	<script type="text/javascript" async="" src="./tmad/ga.js.download"></script>
	<script async="" src="./tmad/prettify.js.download"></script>

	<script type="text/javascript">

		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-22940424-1']);
		_gaq.push(['_trackPageview']);

		(function () {
			var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
		})();

	</script>

</head>

<body>
	<div id="content">
		<div id="content-inner">

			<div class="section head">
				<h1>Texture Memory-Augmented Deep Patch-Based Image Inpainting</h1>

				<div class="authors">
					<a href="https://nbei.github.io/">Rui
						Xu<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://www.minghaoguo.com/">Minghao Guo</a>
					<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://scholar.google.com.hk/citations?user=GDvt570AAAAJ&hl=zh-CN">Jiaqi Wang</a>
					<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="https://scholar.google.com.hk/citations?user=udZam0oAAAAJ&hl=zh-CN">Xiaoxiao
						Li<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="http://bzhou.ie.cuhk.edu.hk/">Bolei Zhou<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
					<a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy<sup>2</sup></a>
				</div>

				<div class="affiliations">
					<sup>1</sup> <a href="http://mmlab.ie.cuhk.edu.hk/" target="_blank">CUHK - SenseTime Joint Lab, The
						Chinese University of Hong Kong</a> <br>
					<sup>2</sup> <a>Nanyang Technological University, Singapore</a> <br>
				</div>

				<div class="venue"></div>
			</div>



			<div class="section abstract">
				<h2>Abstract</h2>
				<br>
				<p>
					Patch-based methods and deep networks have been employed to tackle image inpainting problem, with
					their own strengths and weaknesses. Patch-based methods are capable of restoring a missing region
					with high-quality texture through searching nearest neighbor patches from the unmasked regions.
					However, these methods bring problematic contents when recovering large missing regions. Deep
					networks, on the other hand, show promising results in completing large regions. Nonetheless, the
					results often lack faithful and sharp details that resemble the surrounding area. By bringing
					together the best of both paradigms, we propose a new deep inpainting framework where texture
					generation is guided by a texture memory of patch samples extracted from unmasked regions. The
					framework has a novel design that allows texture memory retrieval to be trained end-to-end with the
					deep inpainting network. In addition, we introduce a patch distribution loss to encourage
					high-quality patch synthesis. The proposed method shows superior performance both qualitatively and
					quantitatively on three challenging image benchmarks, i.e., Places, CelebA-HQ, and Paris Street-View
					datasets.

				</p>
			</div>

			<div class="section framework">
				<h2>Framework</h2>
				<br>
				<center><img src="./tmad/framework-tight-mod-min.png" border="0" width="95%"></center>
				<br>
				<p>
					As shown in this figure, our two-stage framework follows a coarse-to-fine style. Taking the whole
					image
					as input, the first stage predicts the coarse results containing global structure guidance.
					Different from
					previous works, the second stage only adopts corrupted patches in a parallel manner. In a word, our
					T-MAD
					first recovers the coarse structure, and then synthesizes high-quality textures at patch level with
					the help of
					the texture memory.

				</p>
			</div>

			<div class="section visualization">
				<h2>Visualization</h2>
				<br>
				<center><img src="./tmad/places-compare-large-min.png" border="0" width="100%"></center>
				<p>
					The qualitative comparison with existing models. From left to right: Corrupted input image, results
					of PatchMatch, PICNet, Edge-Connect, DeepFill, CRA, our T-MAD and ground-truth.<b> (Best viewed with
						zoom-in.)</b>

			</div>

			<div class="section materials">
				<h2>Materials</h2>
				<center>
					<ul>

						<li class="grid">
							<div class="griditem">
								<a href="https://arxiv.org/pdf/2009.13240.pdf" target="_blank" class="imageLink"><img
										src="./tmad/paper.jpg" border="0" width="50%"></a><br>
								<a href="https://arxiv.org/pdf/2009.13240.pdf"
									target="_blank">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Full Paper
									with
									Appendix</a>
							</div>
						</li>
						&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

						<li class="grid">
							<div class="griditem">
								<a><img src="./gan-pos-encoding/code.png"></a><br>
								<a>Codes and Experiments</a>
							</div>
						</li>

					</ul>
				</center>
			</div>

			<br>

			<div class="section citation">
				<h2>Citation</h2>
				<div class="section bibtex">
					<pre>@article{xu2020texture,
title={Texture Memory-Augmented Deep Patch-Based Image Inpainting},
author={Xu, Rui and Guo, Minghao and Wang, Jiaqi and Li, Xiaoxiao and Zhou, Bolei and Loy, Chen Change},
journal={arXiv preprint arXiv:2009.13240},
year={2020}
}</pre>
				</div>
			</div>

		</div>
		<!--=================Contact==========================-->
		<div class="section contact">
			<h2 id="contact">Contact</h2>
			<p>If you have any question, please contact Rui Xu at <strong>xr018@ie.cuhk.edu.hk</strong>.</p> <br>
		</div>
	</div>

</body>

</html>